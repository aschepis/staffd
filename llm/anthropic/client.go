package anthropic

import (
	"context"
	"encoding/json"
	"fmt"

	anthropic "github.com/anthropics/anthropic-sdk-go"
	"github.com/anthropics/anthropic-sdk-go/option"
	"github.com/aschepis/backscratcher/staff/llm"
	"github.com/rs/zerolog"
)

// AnthropicClient implements the llm.Client interface for Anthropic's API.
type AnthropicClient struct {
	client *anthropic.Client
	logger zerolog.Logger
}

// NewAnthropicClient creates a new AnthropicClient with the given API key.
func NewAnthropicClient(apiKey string, logger zerolog.Logger) (*AnthropicClient, error) {
	if apiKey == "" {
		return nil, fmt.Errorf("api key is required")
	}

	client := anthropic.NewClient(option.WithAPIKey(apiKey))
	return &AnthropicClient{
		client: &client,
		logger: logger,
	}, nil
}

// Synchronous implements llm.Client.Synchronous.
func (c *AnthropicClient) Synchronous(ctx context.Context, req *llm.Request) (*llm.Response, error) {
	if req == nil {
		return nil, fmt.Errorf("request is required")
	}

	// Convert tools
	tools := ToToolUnionParams(req.Tools)

	// Convert messages
	anthropicMsgs, err := ToMessageParams(req.Messages)
	if err != nil {
		return nil, fmt.Errorf("failed to convert messages: %w", err)
	}

	// Build system blocks with prompt caching
	systemBlocks := buildSystemBlocks(req.System)

	// Create API params
	params := anthropic.MessageNewParams{
		Model:     anthropic.Model(req.Model),
		MaxTokens: req.MaxTokens,
		Messages:  anthropicMsgs,
		System:    systemBlocks,
		Tools:     tools,
	}

	// Make API call
	message, err := c.client.Messages.New(ctx, params)
	if err != nil {
		return nil, err
	}

	// Convert response
	content := make([]llm.ContentBlock, 0, len(message.Content))
	for _, blockUnion := range message.Content {
		switch block := blockUnion.AsAny().(type) {
		case anthropic.TextBlock:
			content = append(content, llm.ContentBlock{
				Type: llm.ContentBlockTypeText,
				Text: block.Text,
			})
		case anthropic.ToolUseBlock:
			// Extract input as map[string]interface{}
			var input map[string]interface{}
			if block.Input != nil {
				if inputBytes, err := json.Marshal(block.Input); err == nil {
					if err := json.Unmarshal(inputBytes, &input); err != nil {
						input = make(map[string]interface{})
					}
				} else {
					input = make(map[string]interface{})
				}
			} else {
				input = make(map[string]interface{})
			}
			content = append(content, llm.ContentBlock{
				Type: llm.ContentBlockTypeToolUse,
				ToolUse: &llm.ToolUseBlock{
					ID:    block.ID,
					Name:  block.Name,
					Input: input,
				},
			})
		}
	}

	// Convert usage
	usage := &llm.Usage{
		InputTokens:              message.Usage.InputTokens,
		OutputTokens:             message.Usage.OutputTokens,
		CacheCreationInputTokens: message.Usage.CacheCreationInputTokens,
		CacheReadInputTokens:     message.Usage.CacheReadInputTokens,
	}

	// Log prompt cache information for tracking efficacy
	if usage.CacheCreationInputTokens > 0 || usage.CacheReadInputTokens > 0 {
		cacheEfficiency := float64(0)
		if usage.InputTokens > 0 {
			cacheEfficiency = float64(usage.CacheReadInputTokens) / float64(usage.InputTokens) * 100
		}
		c.logger.Debug().
			Int64("input_tokens", usage.InputTokens).
			Int64("cache_creation_tokens", usage.CacheCreationInputTokens).
			Int64("cache_read_tokens", usage.CacheReadInputTokens).
			Float64("cache_efficiency", cacheEfficiency).
			Msg("Prompt cache stats")
	}

	// Extract stop reason
	stopReason := string(message.StopReason)

	return &llm.Response{
		Content:    content,
		Usage:      usage,
		StopReason: stopReason,
	}, nil
}

// Stream implements llm.Client.Stream.
func (c *AnthropicClient) Stream(ctx context.Context, req *llm.Request) (llm.Stream, error) {
	if req == nil {
		return nil, fmt.Errorf("request is required")
	}

	// Convert tools
	tools := ToToolUnionParams(req.Tools)

	// Convert messages
	anthropicMsgs, err := ToMessageParams(req.Messages)
	if err != nil {
		return nil, fmt.Errorf("failed to convert messages: %w", err)
	}

	// Build system blocks with prompt caching
	systemBlocks := buildSystemBlocks(req.System)

	// Create API params
	params := anthropic.MessageNewParams{
		Model:     anthropic.Model(req.Model),
		MaxTokens: req.MaxTokens,
		Messages:  anthropicMsgs,
		System:    systemBlocks,
		Tools:     tools,
	}

	// Create streaming request
	stream := c.client.Messages.NewStreaming(ctx, params)

	// Create and return our stream wrapper
	return newAnthropicStream(ctx, stream, c.logger), nil
}

// buildSystemBlocks creates system text blocks with prompt caching enabled if appropriate.
// According to Anthropic's prompt caching documentation, placing cache_control on the system block
// caches the full prefix: tools, system, and messages (in that order) up to and including the
// block designated with cache_control. This means tools are automatically cached along with the system prompt.
//
// Prompt caching is enabled when the combined size of tools + system is at least 4000 characters
// (roughly equivalent to ~1000 tokens, meeting Anthropic's 1024 token minimum requirement).
// This helps reduce costs and latency for repeated requests with the same tools and system prompt.
func buildSystemBlocks(systemPrompt string) []anthropic.TextBlockParam {
	blocks := []anthropic.TextBlockParam{
		{Text: systemPrompt, CacheControl: anthropic.NewCacheControlEphemeralParam()},
	}

	return blocks
}
